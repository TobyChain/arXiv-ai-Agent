# ArXiv AI Daily Report (2025-12-20)

- 论文数：1

---
## 1. Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning

- 作者：Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille
- 子主题：大模型（LLM）与强化学习
- 推荐：很推荐
- 关键词：对抗性强化学习, LLM推理增强, 步骤级奖励（step-level reward）
- Abstract：http://arxiv.org/abs/2512.16917v1
- PDF：https://arxiv.org/pdf/2512.16917v1

**中文摘要**

大型语言模型（LLM）具备显式推理能力，在数学推理等任务上表现优越，但仍容易出现过程性错误，如计算错误、脆弱的逻辑链以及看似合理但实际无效的步骤。本文提出了Generative Adversarial Reasoner（生成对抗推理器），一种基于on-policy的联合训练框架，通过对抗性强化学习共同进化一个LLM推理器与一个基于LLM的判别器以提升推理能力。为提高计算效率，设计了一种复核调度（review schedule），将每条推理链划分为若干长度相近且逻辑上完整的片段；判别器对每个片段的合理性进行评估，并给出简洁结构化的论证。训练中，推理器根据产生逻辑一致且能得到正确答案的步骤获得奖励，而判别器则因正确检测错误或区分推理痕迹而获得奖励。该设计生成了密集且校准良好的on-policy步骤级奖励，以补充稀疏的精确匹配信号，从而改善归因（credit assignment）、提高样本效率并增强LLM的整体推理质量。在多种数学基准测试上，该方法在标准的RL后训练下持续优于强基线。具体地，在AIME24数据集上，DeepSeek-R1-Distill-Qwen-7B从54.0提升到61.3（+7.3），DeepSeek-R1-Distill-Llama-8B从43.7提升到53.7（+10.0）。此外，模块化的判别器还支持灵活的奖励塑形（reward shaping），可用于教师蒸馏、偏好对齐以及基于数学证明的推理等目标。

---
