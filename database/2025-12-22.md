---
## 1. Towards Explainable Conversational AI for Early Diagnosis with Large Language Models

- 作者：Maliha Tabassum, M Shamim Kaiser
- 子主题：LLM
- 推荐：极度推荐
- 关键词：医疗对话诊断, 大型语言模型(LLM), 可解释性AI
- Abstract：http://arxiv.org/abs/2512.17559v1
- PDF：https://arxiv.org/pdf/2512.17559v1

**中文摘要**

全球医疗系统正面临诊断效率低下、成本上升和专科医生资源有限等问题，导致治疗延误和不良健康结局。当前多数基于AI和深度学习的诊断系统交互性和透明性不足，在以患者为中心的真实场景中效果有限。本研究提出了一种基于大型语言模型（采用GPT-4o）、检索增强生成（RAG）与可解释性AI技术的诊断型聊天机器人。该聊天机器人通过动态对话与患者交互，提取并标准化症状信息，结合相似性匹配与自适应问询对潜在诊断进行优先排序；并通过Chain-of-Thought提示提供更透明的诊断推理过程。与朴素贝叶斯、逻辑回归、支持向量机、随机森林和KNN等传统机器学习模型比较，所提出的LLM系统表现优异，达到90%的准确率和100%的Top-3准确率。研究结果为在医疗场景中实现更具透明性、交互性和临床相关性的AI诊断系统提供了积极前景。

---
## 2. Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation

- 作者：Daksh Jain, Aarya Jain, Ashutosh Desai, Avyakt Verma, Ishan Bhanuka, Pratik Narang, Dhruv Kumar
- 子主题：LLM
- 推荐：很推荐
- 关键词：大语言模型, 策略推理, 程序化内容生成
- Abstract：http://arxiv.org/abs/2512.17308v1
- PDF：https://arxiv.org/pdf/2512.17308v1

**中文摘要**

宝可梦对战中的策略决策为评估大型语言模型提供了独特的试验场。宝可梦对战要求对属性相克、统计学权衡和风险评估等因素进行推理，这些能力与人类的策略性思维相似。本研究考察大型语言模型能否作为称职的对战代理，既能做出战术上合理的决策，又能生成新颖且平衡的游戏内容。我们构建了一个回合制的宝可梦对战系统，使模型基于当前战况选择招式，而非依赖预先编程的逻辑。该框架囊括了关键的宝可梦机制：属性相性倍数、基于属性的伤害计算以及多宝可梦队伍管理。通过对多种模型架构的系统性评估，我们测量了胜率、决策延迟、属性对齐精度以及令牌（token）效率等指标。结果表明，大型语言模型无需领域专门训练即可作为动态的游戏对手，提供了一种面向回合制策略游戏的、可行的替代强化学习的方法。其兼具战术推理与内容创生的双重能力，使得大型语言模型既可作为玩家也可作为设计者，对于程序化生成和自适应难度系统在互动娱乐中的应用具有重要意义。

---
## 3. About Time: Model-free Reinforcement Learning with Timed Reward Machines

- 作者：Anirban Majumdar, Ritam Raha, Rajarshi Roy, David Parker, Marta Kwiatkowska
- 子主题：RL
- 推荐：很推荐
- 关键词：定时奖励机, 非马尔可夫奖励, 模型无关强化学习
- Abstract：http://arxiv.org/abs/2512.17637v1
- PDF：https://arxiv.org/pdf/2512.17637v1

**中文摘要**

奖励函数的说明在强化学习（RL）中起着核心作用，指导智能体的行为。为表达非马尔可夫奖励，已有诸如奖励机（reward machines）等形式化方法用于捕捉对历史的依赖。然而，传统的奖励机无法建模精确的时序约束，限制了其在对时间敏感的应用中的适用性。本文提出了定时奖励机（Timed Reward Machines, TRMs），将时序约束引入奖励结构，从而扩展了奖励机的表达能力。TRM 支持可调的奖励逻辑，例如对延迟施加代价或对及时行为给予奖励。我们研究了在数字语义和实时语义下，基于模型无关的强化学习框架（如表格 Q-learning）中学习 TRM 下最优策略的方法。所提出的算法通过对定时自动机的抽象将 TRM 整合到学习过程中，并采用利用 TRM 结构的反事实想象启发式方法以改进搜索效率。实验表明，我们的算法能够学习出在流行强化学习基准上既获得高回报又满足 TRM 指定时序约束的策略。此外，我们还比较了不同 TRM 语义下的性能，并通过消融实验突出了反事实想象机制的优势。

---
## 4. Translating the Rashomon Effect to Sequential Decision-Making Tasks

- 作者：Dennis Gross, Jørn Eirik Betten, Helge Spieker
- 子主题：RL
- 推荐：很推荐
- 关键词：Rashomon效应, 强化学习（序列决策）, 形式化验证与鲁棒性
- Abstract：http://arxiv.org/abs/2512.17470v1
- PDF：https://arxiv.org/pdf/2512.17470v1

**中文摘要**

Rashomon效应描述了在相同数据上训练的多个模型在预测上表现相同但在内部依赖的特征上存在差异的现象。该效应已在分类任务中得到广泛研究，但尚未扩展到序列决策领域，在该领域智能体通过在环境中采取动作学习策略以实现目标。本文将Rashomon效应引入序列决策问题，定义为多种策略在行为上等价——访问相同状态并选择相同动作——但在内部结构（例如特征归因）上存在差异。序列决策中验证行为等价性不同于分类任务：在分类中预测可以直接与真实标签比较，而在具有随机转移的序列决策中，由于随机性，同一策略在单条轨迹上可能成功或失败。为此，我们采用形式化验证方法，构建并比较每个策略在环境中的完整概率行为。实验证明Rashomon效应在序列决策中确实存在。我们进一步展示，从Rashomon集合构建的策略集成在分布转移下比单一策略具有更强的鲁棒性；此外，从Rashomon集合导出的宽容（permissive）策略在保持最优性能的同时降低了验证的计算开销。

---
## 5. Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally

- 作者：Robin Schimmelpfennig, Mark Díaz, Vinodkumar Prabhakaran, Aida Davani
- 子主题：Human-AI Interaction (HCI)
- 推荐：很推荐
- 关键词：拟人化, 人机交互, 跨文化差异
- Abstract：http://arxiv.org/abs/2512.17898v1
- PDF：https://arxiv.org/pdf/2512.17898v1

**中文摘要**

全球有超过十亿用户与越来越多被设计为模仿人类特质的人工智能系统交互。这一趋势激发了关于拟人化（即将人类特征归于人工代理）的紧迫讨论，人们担忧它可能导致错误的信任或情感依赖。然而，至今尚未在具有全球用户样本的真实人机交互情境中，实证检验更类人人工智能设计与后续参与度和信任之间的因果关系。现有的安全框架多基于西方人群的理论假设，忽视了全球用户的多样性。为填补这些空白，我们通过两项大规模跨国实验（N=3500）在10个不同国家中开展了实时、开放式的人机交互研究。我们发现，在评估AI的“类人程度”时，用户关注的并不是政策讨论中常提及的理论性要素（如有感知或意识），而是诸如对话流畅性或是否理解用户视角等更为应用性、交互性的线索。我们还通过实验性操控证明，类人设计可以因果性地提高用户的拟人化归因；但与既有理论工作所预期的不同，类人设计并不会普遍提升用户的行为性参与或信任。相反，类人程度与行为性结果之间的部分关联受到文化因素的断裂性影响：在某些人群（如巴西），某些设计选择会增强用户自述的信任，而在另一些人群（如日本）则可能产生相反效果。我们的发现挑战了关于类人AI设计固有风险的主流叙事，提示人机交互存在细致且受文化调节的格局，要求在AI治理中超越一刀切的做法。

---
## 6. ScoutGPT: Capturing Player Impact from Team Action Sequences Using GPT-Based Framework

- 作者：Miru Hong, Minho Lee, Geonhee Jo, Jae-Hee So, Pascal Bauer, Sang-Ki Ko
- 子主题：LLM
- 推荐：很推荐
- 关键词：事件序列建模, 反事实模拟, 球员嵌入
- Abstract：http://arxiv.org/abs/2512.17266v1
- PDF：https://arxiv.org/pdf/2512.17266v1

**中文摘要**

转会在决定足球俱乐部成败中起着关键作用，但由于场上表现高度依赖情境，预测转会是否成功仍然困难。现有的评估实践常依赖静态汇总统计或事后价值模型，无法反映球员在新战术环境或不同队友下贡献的适应性。为填补这一空白，我们提出了 EventGPT，一种基于 GPT 风格自回归变换器的、以球员为条件且具有价值感知的下一事件预测模型。该模型将比赛视为离散的 token 序列，联合学习在给定前文语境与球员身份的条件下预测下一次有球动作的类型、位置、时机以及其估计的残差球上价值（residual On-Ball Value, rOBV）。该框架的一个关键贡献是能够进行反事实模拟：通过将学得的球员嵌入替换到新的事件序列中，模型可以模拟球员在不同球队或战术结构下的行为分布与价值变化。在英超五个赛季的事件数据上评估，EventGPT 在下一事件预测的准确性与空间精度方面均优于现有基于序列的方法。此外，通过若干案例研究（例如比较不同体系下的前锋表现、为特定角色识别风格相近的替代者），展示了该方法在转会分析中评估适配性的实用价值和原则性方法。

---
## 7. When Reasoning Meets Its Laws

- 作者：Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin, Paul Pu Liang, Huan Zhang
- 子主题：LLM
- 推荐：很推荐
- 关键词：推理定律, 可组合性, 大规模推理模型
- Abstract：http://arxiv.org/abs/2512.17901v1
- PDF：https://arxiv.org/pdf/2512.17901v1

**中文摘要**

尽管大规模推理模型（LRMs）在性能上表现优异，它们的推理行为常常反直觉，从而导致推理能力未达最佳化。为在理论上刻画期望的推理行为，本文提出了“推理定律”（LoRe），一个统一框架，用以描述大规模推理模型中固有的推理模式。我们首先提出了计算定律（compute law），假设推理所需的计算资源应与问题复杂度呈线性尺度增长。除了计算定律外，我们还将LoRe扩展为包含补充的准确性定律。由于问题复杂度在实践中难以量化，我们通过两个可测属性——单调性和可组合性——来检验这些假设。为此，我们引入了LoRe-Bench基准，用以系统性地衡量大型推理模型在这两个可控属性上的表现。评估结果表明，大多数推理模型在单调性方面表现尚可，但在可组合性方面存在不足。针对这一问题，我们提出了一种有效的微调方法，用以加强模型对计算定律的可组合性约束。大量实证研究表明，更好地遵循计算定律能在多个基准上持续提升推理性能，并揭示了属性与定律之间的协同效应。项目主页： https://lore-project.github.io/

---
## 8. Dialectics for Artificial Intelligence

- 作者：Zhengmian Hu
- 子主题：表示学习/概念学习
- 推荐：很推荐
- 关键词：算法信息论, 概念学习, 多智能体对齐
- Abstract：http://arxiv.org/abs/2512.17373v1
- PDF：https://arxiv.org/pdf/2512.17373v1

**中文摘要**

人工智能能否从原始经验中、在没有人为监督的情况下，发现出人类已经发现的概念？一个挑战是人类概念本身具有流动性：随着探究的推进，概念边界会发生移动、分裂和合并（例如冥王星不再被视为行星）。为取得进展，需要一个不仅仅是词典标签的“概念”定义——它应当是可修订、可比较、并可在智能体之间对齐的结构。本文提出一种基于算法信息论的视角，将概念视为仅通过其与智能体全集经验的结构性关系所定义的信息对象。核心约束是“决定性”：若一组部分构成可逆的一致性关系，则任一缺失部分可以（在Kolmogorov式恒等中的对数松弛范围内）从其余部分恢复回来。这种可逆性阻止了“概念”脱离经验独立存在，并把概念的存在性转化为可检验的结构性命题。为判定某种分解是否自然，我们定义了“额外信息”，用于度量将经验划分为多个独立描述部分时引入的冗余开销。在这些定义之上，我们把辩证过程公式化为一种优化动力学：当新的信息片段出现或受到争议时，竞争的概念会通过更短的条件化描述来争取解释这些片段，从而驱动概念的系统性扩展、收缩、分裂与合并。最后，我们形式化了低成本的概念传递和多智能体对齐，使用小的基础/种子使另一个智能体在共享协议下重构相同概念，将通信具体化为计算与比特数之间的权衡。

---
